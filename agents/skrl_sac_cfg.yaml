# agents/skrl_sac_cfg.yaml
seed: 42

# Memory
memory:
  memory_size: 100000  # Replay buffer (SAC off-policy)

# Models
models:
  actor:
    class: "GaussianMixin"
    layers: [256, 256, 256]
    activation: "elu"
  critic_1:
    class: "DeterministicMixin"
    layers: [256, 256, 256]
    activation: "elu"
  critic_2:  # Twin Q-networks
    class: "DeterministicMixin"
    layers: [256, 256, 256]
    activation: "elu"

# Algorithm
agent:
  class: "SAC"
  # Learning rates
  actor_learning_rate: 3.0e-4
  critic_learning_rate: 1.0e-3
  # SAC specific
  learn_entropy: true
  entropy_learning_rate: 3.0e-4
  initial_entropy_value: 0.2
  target_entropy: "auto"  # -dim(action_space)
  # Training
  discount_factor: 0.99
  polyak: 0.005  # Soft update tau
  batch_size: 256

# Trainer
trainer:
  class: "SequentialTrainer"
  timesteps: 1000000
  environment_info: "log"