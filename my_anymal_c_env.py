# Copyright (c) 2022-2025, The Isaac Lab Project Developers
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
#
# MODIFIED: Added velocity command visualization arrows

from __future__ import annotations

import gymnasium as gym
import torch

import isaaclab.sim as sim_utils
from isaaclab.assets import Articulation
from isaaclab.envs import DirectRLEnv
from isaaclab.sensors import ContactSensor, RayCaster

# ============ MARKER IMPORTS ============
from isaaclab.markers import VisualizationMarkers, VisualizationMarkersCfg
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR
import isaaclab.utils.math as math_utils
# ========================================

from .my_anymal_c_env_cfg import MyAnymalFlatEnvCfg, MyAnymalRoughEnvCfg


def define_velocity_markers() -> VisualizationMarkersCfg:
    """Define markers for velocity command visualization."""
    marker_cfg = VisualizationMarkersCfg(
        prim_path="/Visuals/VelocityMarkers",
        markers={
            "velocity_command": sim_utils.UsdFileCfg(
                usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/UIElements/arrow_x.usd",
                scale=(0.5, 0.5, 1.0),
                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),  # Red
            ),
            "velocity_actual": sim_utils.UsdFileCfg(
                usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/UIElements/arrow_x.usd",
                scale=(0.4, 0.4, 0.8),
                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),  # Green
            ),
            "heading": sim_utils.UsdFileCfg(
                usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/UIElements/arrow_x.usd",
                scale=(0.3, 0.3, 0.6),
                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 1.0)),  # Cyan
            ),
        },
    )
    return marker_cfg


class MyAnymalEnv(DirectRLEnv):
    cfg: MyAnymalFlatEnvCfg | MyAnymalRoughEnvCfg

    def __init__(self, cfg: MyAnymalFlatEnvCfg | MyAnymalRoughEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)

        self._actions = torch.zeros(self.num_envs, gym.spaces.flatdim(self.single_action_space), device=self.device)
        self._previous_actions = torch.zeros(
            self.num_envs, gym.spaces.flatdim(self.single_action_space), device=self.device
        )

        self._commands = torch.zeros(self.num_envs, 3, device=self.device)

        # ============ MARKER SETUP ============
        self._marker_offset = torch.tensor([0.0, 0.0, 0.7], device=self.device)
        self._up_vec = torch.tensor([0.0, 0.0, 1.0], device=self.device).repeat(self.num_envs, 1)
        # Default scales for markers
        self._marker_scale = torch.ones(self.num_envs, 3, device=self.device)
        # ======================================

        self._episode_sums = {
            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
            for key in [
                "forward_bonus",
                "track_lin_vel_xy_exp",
                "track_ang_vel_z_exp",
                "lin_vel_z_l2",
                "ang_vel_xy_l2",
                "dof_torques_l2",
                "dof_acc_l2",
                "action_rate_l2",
                "feet_air_time",
                "undesired_contacts",
                "flat_orientation_l2",
            ]
        }
        self._base_id, _ = self._contact_sensor.find_bodies("base")
        self._feet_ids, _ = self._contact_sensor.find_bodies(".*FOOT")
        self._undesired_contact_body_ids, _ = self._contact_sensor.find_bodies(".*THIGH")

    def _setup_scene(self):
        self._robot = Articulation(self.cfg.robot)
        self.scene.articulations["robot"] = self._robot
        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
        self.scene.sensors["contact_sensor"] = self._contact_sensor

        if isinstance(self.cfg, MyAnymalRoughEnvCfg):
            self._height_scanner = RayCaster(self.cfg.height_scanner)
            self.scene.sensors["height_scanner"] = self._height_scanner

        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
        self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)

        self.scene.clone_environments(copy_from_source=False)

        if self.device == "cpu":
            self.scene.filter_collisions(global_prim_paths=[self.cfg.terrain.prim_path])

        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
        light_cfg.func("/World/Light", light_cfg)

        # ============ CREATE VISUALIZATION MARKERS ============
        self._velocity_markers = VisualizationMarkers(define_velocity_markers())
        # ======================================================

    def _pre_physics_step(self, actions: torch.Tensor):
        self._actions = actions.clone()
        self._processed_actions = self.cfg.action_scale * self._actions + self._robot.data.default_joint_pos

    def _apply_action(self):
        self._robot.set_joint_position_target(self._processed_actions)

    def _get_observations(self) -> dict:
        self._previous_actions = self._actions.clone()
        height_data = None

        if isinstance(self.cfg, MyAnymalRoughEnvCfg):
            height_data = (
                    self._height_scanner.data.pos_w[:, 2].unsqueeze(1) -
                    self._height_scanner.data.ray_hits_w[..., 2] - 0.5
            ).clip(-1.0, 1.0)

        obs = torch.cat(
            [
                tensor
                for tensor in (
                self._robot.data.root_lin_vel_b,
                self._robot.data.root_ang_vel_b,
                self._robot.data.projected_gravity_b,
                self._commands,
                self._robot.data.joint_pos - self._robot.data.default_joint_pos,
                self._robot.data.joint_vel,
                height_data,
                self._actions,
            )
                if tensor is not None
            ],
            dim=-1,
        )
        observations = {"policy": obs}

        # ============ VISUALIZE MARKERS ============
        self._visualize_velocity_markers()
        # ===========================================

        return observations

    def _visualize_velocity_markers(self):
        """Visualize velocity command and actual velocity as arrows above each robot."""
        base_pos_w = self._robot.data.root_pos_w
        base_quat_w = self._robot.data.root_quat_w

        marker_pos = base_pos_w + self._marker_offset

        # 1. COMMAND VELOCITY ARROW (Red)
        cmd_vx = self._commands[:, 0]
        cmd_vy = self._commands[:, 1]
        cmd_yaw_body = torch.atan2(cmd_vy, cmd_vx)

        # Extract robot yaw from quaternion (w, x, y, z)
        qw, qx, qy, qz = base_quat_w[:, 0], base_quat_w[:, 1], base_quat_w[:, 2], base_quat_w[:, 3]
        robot_yaw = torch.atan2(2.0 * (qw * qz + qx * qy), 1.0 - 2.0 * (qy * qy + qz * qz))

        cmd_yaw_world = robot_yaw + cmd_yaw_body
        cmd_quat_world = math_utils.quat_from_angle_axis(cmd_yaw_world, self._up_vec)

        # 2. ACTUAL VELOCITY ARROW (Green)
        actual_vel_w = self._robot.data.root_lin_vel_w[:, :2]
        actual_vx = actual_vel_w[:, 0]
        actual_vy = actual_vel_w[:, 1]
        actual_yaw = torch.atan2(actual_vy, actual_vx)
        actual_quat = math_utils.quat_from_angle_axis(actual_yaw, self._up_vec)

        # 3. HEADING ARROW (Cyan)
        heading_quat = base_quat_w

        # Stack positions
        offset_actual = torch.tensor([0.0, 0.0, 0.1], device=self.device)
        offset_heading = torch.tensor([0.0, 0.0, 0.2], device=self.device)

        all_positions = torch.cat([
            marker_pos,
            marker_pos + offset_actual,
            marker_pos + offset_heading,
        ], dim=0)  # (num_envs * 3, 3)

        all_orientations = torch.cat([
            cmd_quat_world,
            actual_quat,
            heading_quat,
        ], dim=0)  # (num_envs * 3, 4)

        # Create marker indices: 0=command(red), 1=actual(green), 2=heading(cyan)
        marker_indices = torch.cat([
            torch.zeros(self.num_envs, dtype=torch.long, device=self.device),
            torch.ones(self.num_envs, dtype=torch.long, device=self.device),
            torch.full((self.num_envs,), 2, dtype=torch.long, device=self.device),
        ], dim=0)

        # Create scales tensor (num_envs * 3, 3) - all ones for default scale
        all_scales = torch.ones(self.num_envs * 3, 3, device=self.device)

        # Visualize all markers with scales
        self._velocity_markers.visualize(
            translations=all_positions,
            orientations=all_orientations,
            scales=all_scales,
            marker_indices=marker_indices
        )

    def _get_rewards(self) -> torch.Tensor:
        lin_vel_error = torch.sum(torch.square(self._commands[:, :2] - self._robot.data.root_lin_vel_b[:, :2]), dim=1)
        lin_vel_error_mapped = torch.exp(-lin_vel_error / 0.25)

        yaw_rate_error = torch.square(self._commands[:, 2] - self._robot.data.root_ang_vel_b[:, 2])
        yaw_rate_error_mapped = torch.exp(-yaw_rate_error / 0.25)

        z_vel_error = torch.square(self._robot.data.root_lin_vel_b[:, 2])
        ang_vel_error = torch.sum(torch.square(self._robot.data.root_ang_vel_b[:, :2]), dim=1)
        joint_torques = torch.sum(torch.square(self._robot.data.applied_torque), dim=1)
        joint_accel = torch.sum(torch.square(self._robot.data.joint_acc), dim=1)
        action_rate = torch.sum(torch.square(self._actions - self._previous_actions), dim=1)

        first_contact = self._contact_sensor.compute_first_contact(self.step_dt)[:, self._feet_ids]
        last_air_time = self._contact_sensor.data.last_air_time[:, self._feet_ids]
        air_time = torch.sum((last_air_time - 0.5) * first_contact, dim=1) * (
                torch.norm(self._commands[:, :2], dim=1) > 0.1
        )

        net_contact_forces = self._contact_sensor.data.net_forces_w_history
        is_contact = (
                torch.max(torch.norm(net_contact_forces[:, :, self._undesired_contact_body_ids], dim=-1), dim=1)[
                    0] > 1.0
        )
        contacts = torch.sum(is_contact, dim=1)

        flat_orientation = torch.sum(torch.square(self._robot.data.projected_gravity_b[:, :2]), dim=1)

        forward_vel = self._robot.data.root_lin_vel_b[:, 0]
        forward_bonus = torch.clamp(forward_vel, min=0.0, max=2.0) * 0.5

        rewards = {
            "forward_bonus": forward_bonus * self.step_dt,
            "track_lin_vel_xy_exp": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale * self.step_dt,
            "track_ang_vel_z_exp": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale * self.step_dt,
            "lin_vel_z_l2": z_vel_error * self.cfg.z_vel_reward_scale * self.step_dt,
            "ang_vel_xy_l2": ang_vel_error * self.cfg.ang_vel_reward_scale * self.step_dt,
            "dof_torques_l2": joint_torques * self.cfg.joint_torque_reward_scale * self.step_dt,
            "dof_acc_l2": joint_accel * self.cfg.joint_accel_reward_scale * self.step_dt,
            "action_rate_l2": action_rate * self.cfg.action_rate_reward_scale * self.step_dt,
            "feet_air_time": air_time * self.cfg.feet_air_time_reward_scale * self.step_dt,
            "undesired_contacts": contacts * self.cfg.undesired_contact_reward_scale * self.step_dt,
            "flat_orientation_l2": flat_orientation * self.cfg.flat_orientation_reward_scale * self.step_dt,
        }

        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)

        for key, value in rewards.items():
            self._episode_sums[key] += value
        return reward

    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
        time_out = self.episode_length_buf >= self.max_episode_length - 1
        net_contact_forces = self._contact_sensor.data.net_forces_w_history
        died = torch.any(torch.max(torch.norm(net_contact_forces[:, :, self._base_id], dim=-1), dim=1)[0] > 1.0, dim=1)
        return died, time_out

    def _reset_idx(self, env_ids: torch.Tensor | None):
        if env_ids is None or len(env_ids) == self.num_envs:
            env_ids = self._robot._ALL_INDICES

        self._robot.reset(env_ids)
        super()._reset_idx(env_ids)

        if len(env_ids) == self.num_envs:
            self.episode_length_buf[:] = torch.randint_like(self.episode_length_buf, high=int(self.max_episode_length))

        self._actions[env_ids] = 0.0
        self._previous_actions[env_ids] = 0.0

        self._commands[env_ids] = torch.zeros_like(self._commands[env_ids]).uniform_(-1.0, 1.0)

        command_magnitude = torch.norm(self._commands[env_ids, :2], dim=1, keepdim=True)
        mask = command_magnitude < 0.3
        self._commands[env_ids, :2] = torch.where(
            mask.expand_as(self._commands[env_ids, :2]),
            self._commands[env_ids, :2] * 2.0,
            self._commands[env_ids, :2]
        )

        joint_pos = self._robot.data.default_joint_pos[env_ids]
        joint_vel = self._robot.data.default_joint_vel[env_ids]
        default_root_state = self._robot.data.default_root_state[env_ids]
        default_root_state[:, :3] += self._terrain.env_origins[env_ids]

        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)

        extras = dict()
        for key in self._episode_sums.keys():
            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])
            extras["Episode_Reward/" + key] = episodic_sum_avg / self.max_episode_length_s
            self._episode_sums[key][env_ids] = 0.0

        self.extras["log"] = dict()
        self.extras["log"].update(extras)

        extras = dict()
        extras["Episode_Termination/base_contact"] = torch.count_nonzero(self.reset_terminated[env_ids]).item()
        extras["Episode_Termination/time_out"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()
        self.extras["log"].update(extras)